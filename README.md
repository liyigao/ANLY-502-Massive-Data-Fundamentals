# ANLY 502 Massive Data Fundamentals

Assignments Repo for Georgetown University Spring 2018 ANLY 502 Massive Data Fundamentals

## Assignments

**Assignment 1** - Linux and AWS Tutorial  
**Assignment 2** - Hadoop Streaming & MapReduce  
**Assignment 3** - Pig and Hive, SQL Tutorial  
**Assignment 4** - Resilient Distributed Datasets (RDDs) & SparkSQL  
**Assignment 5** - Machine Learning with Spark

## Course Description

Data is everywhere\! Many times, itâ€™s just too big to work with
traditional tools. This is a hands-on, practical workshop style course
about using cloud computing resources to do analysis and manipulation of
datasets that are too large to fit on a single machine and/or analyzed
with traditional tools. The course will focus on Spark, MapReduce, the
Hadoop Ecosystem and other tools.

You will understand how to acquire and/or ingest the data, and then
massage, clean, transform, analyze, and model it within the context of
big data analytics. You will be able to think more programmatically and
logically about your big data needs, tools and issues.

## Course Objectives

  - Operate big data tools and cloud infrastructure, including Spark,
    MapReduce, Hadoop and other tools in the big data ecosystem
  - Recognize and use ancillary tools that support big data processing,
    including git and the Linux command line
  - Setup and manage big data infrastructure and tools in the cloud on
    Amazon Web Services and Microsoft Azure
  - Identify broad spectrum resources and documentation to remain
    current with big data tools and developments
  - Execute a big data analytics exercise from start to finish: ingest,
    wrangle, clean, analyze and store
  - Be aware of the responsibilities that are associated with performing
    analysis of large datasets
